  ---
title: "Intro to Machine Learning with R and Caret"
output: word_document
---

```{R}
library(caret)
library(doSNOW)
#train<-read.csv("C:/Users/iceti/Documents/GitHub/Data-Analyst/Machine_Learning_Caret/train.csv")
train<-read.csv("C:/Users/kun hu/Documents/GitHub/Data-Analyst/Machine_Learning_Caret/train.csv")
#View(train)

```
Replace missing embardked values with mode
```{R}
table(train$Embarked)
train$Embarked[train$Embarked==""]<-"S"
```
Add a feature for tracking missing ages
```{r}
summary(train$Age)
train$missingAge<-ifelse(is.na(train$Age),"Y","N")
```
Add a feature for family size
```{r}
train$FamilySize<-1+train$SibSp+train$Parch
```

Set up factor

```{r}
train$Survived<-as.factor(train$Survived)
train$Pclass<-as.factor(train$Pclass)
train$Sex<-as.factor(train$Sex)
train$Embarked<-as.factor(train$Embarked)
```
Subset data to feature we wish to keep/use
```{r}
features<-c("Survived","Pclass","Sex","Age","SibSp",
            "Parch","Fare","Embarked","missingAge",
            "FamilySize")
train<-train[,features]
str(train)
```

 First, tranform all feature to dummy variables
```{r}
dummy.vars<-dummyVars(~.,data=train[,-1])
train.dummy<-predict(dummy.vars,train[,-1])
```
Impute age
```{r}
pre.process<-preProcess(train.dummy,method="bagImpute")
imputed.data<-predict(pre.process,train.dummy)
#impute age to original data
train$Age<-imputed.data[,6]

```

Use caret to create a 70/30%split of the training data,
keeping rhe proportions of the Survived class label the same across the splits

```{r}
set.seed(54321)
indexes<-createDataPartition(train$Survived,time=1,p=0.7,list=FALSE)
titanic.train<-train[indexes,]
titanic.test<-train[-indexes,]

```

Examine the proportions of survived class lable across the datasets
```{r}
prop.table(table(train$Survived))
prop.table(table(titanic.train$Survived))
prop.table(table(titanic.test$Survived))
```
Set up caret to perform 10-fold cross validation repeated 3 times and to use a grid search for optimal model hyperparameter
```{r}
train.control<- trainControl(method="repeatedcv",
                              number=10,
                              repeats=3,
                              search="grid")
```

Leverage a grid search of hyperparameters for xgboost.
```{R}
tune.grid<-expand.grid(eta=c(0.05,0.075,0.1),
                       nrounds=c(50,75,100),
                       max_depth=6:8,
                       min_child_weight=c(2.0,2.25,2.5),
                       colsample_bytree=c(0.3,0.4,0.5),
                       gamma=0,
                       subsample=1)
```

```{r}
library(xgboost)
cl<-makeCluster(10,type="SOCK")
#regeister cluster so that caret will know to train in parallel
registerDoSNOW(cl)
#train the xgboost model using 10-fold CV repeated 3 times
#and a hyperparameter grid search to retain the optimal model
caret.cv<-train(Survived~.,
                data=titanic.train,
                method="xgbTree",
                tuneGrid=tune.grid,
                trControl=train.control)
stopCluster(cl)
```

```{r}
#Examine 
caret.cv


```